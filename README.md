# Advanced Pokemon TCG AI

This project implements a state-of-the-art Reinforcement Learning (RL) agent for the Pokemon Trading Card Game (TCG). It combines **AlphaZero-style Monte Carlo Tree Search (MCTS)** with **Genetic Algorithms (GA)** and **Population-Based Training (PBT)** to evolve highly strategic agents.

## üöÄ Key Features

*   **AlphaZero Engine**: tailored implementation of MCTS with a Transformer-based policy-value network.
*   **Genetic Algorithm**: Evolve a population of agents with mutation, crossover, and elitism to avoid local optima.
*   **Parallel Training**: Multiprocessing support to scale data generation across CPU cores.
*   **League System**: Agents train against a historical "League" of past versions to prevent transitive cycles (rock-paper-scissors loops).
*   **Advanced Reward Shaping**: Heuristics to guide exploration in the complex TCG action space (favoring evolution, attacking, and strategic setup).
*   **Rich Visualization**: Live training dashboards and detailed strategy analysis tools.

---

## üìÇ Project Structure

### 1. Training & Agents
*   **`train_advanced.py`**: **[MAIN]** The flagship training script. Integrates MCTS, GA, League, and Parallelism.
*   **`train_alphazero.py`**: Legacy single-agent AlphaZero training script.
*   **`play_mcts.py`**: Inference script for the agent to play against itself or a human (CLI).
*   **`alpha_rank.py`**: Implementation of AlphaRank for theoretically sound multi-agent evaluation.

### 2. Core Game Engine (`tcg/`)
*   **`tcg/env.py`**: The `PTCGEnv` class. This is the OpenAI Gym-compatible environment that manages game state, turns, and rules.
*   **`tcg/mcts.py`**: The Monte Carlo Tree Search implementation. Handles simulation, selection (PUCT), and backpropagation.
*   **`tcg/cards.py`**: The card database (`CARD_REGISTRY`). Defines HP, types, attacks, and costs.
*   **`tcg/effects.py`**: The logic for ALL card effects (attacks, abilities, trainers). This is the "brain" of the rules engine.
*   **`tcg/actions.py`**: Defines the discrete action space (873+ possible actions) and handles masking invalid moves.
*   **`tcg/state.py`**: Converts the complex game state object into a tensor representation (`obs_dim=229`) for the neural network. Uses **bag-of-words encoding** for hand visibility (77 card vocabulary).

### 3. Analysis & Visualization
*   **`plot_training.py`**: **[LIVE]** Real-time dashboard for training metrics. Auto-detects learning events and strategy shifts.
*   **`analyze_strategies_v2.py`**: Deep dive analysis of trained agents. Generates JSON reports on card usage, win conditions, and key turns.
*   **`strategy_visualizer.html`**: Interactive web-based viewer for the JSON reports generated by the analysis tools.
*   **`record_replay.py`**: Helper to record games into a JSON format.
*   **`replay_viewer.html`**: Web-based replay viewer to watch games move-by-move.

### 4. Development Tools
*   **`inspect_card.py`**: Diagnostics tool to view the exact code implementation, edge cases, and known interactions for any card.
*   **`verify_cards.py`**: Batch verification script to ensure cards function correctly.

---

## üõ†Ô∏è How to Run

### 1. Train the Agent
To start the advanced training loop with parallel workers:

```bash
# Recommended for 8-core CPU
python train_advanced.py --episodes 5000 --num_workers 6 --population 10

# Recommended for 16-core CPU
python train_advanced.py --episodes 10000 --num_workers 12 --mcts_sims 50
```

**Key Arguments:**

| Argument | Default | Description |
|----------|---------|-------------|
| `--episodes N` | 5000 | Total episodes to train |
| `--mcts_sims N` | 50 | MCTS simulations per move (30-100 recommended) |
| `--batch_size N` | 256 | Training batch size |
| `--num_workers N` | 4 | Number of parallel game workers |

**Population & Genetic Algorithm:**

| Argument | Default | Description |
|----------|---------|-------------|
| `--population N` | 5 | Size of agent population for GA |
| `--no_pbt` | False | Disable Population-Based Training / Genetic Algorithm |
| `--mutation_rate F` | 0.05 | Probability of mutating each weight (0-1) |
| `--mutation_strength F` | 0.005 | Standard deviation of mutation noise |
| `--crossover_rate F` | 0.2 | Probability of crossover vs pure mutation |
| `--elitism N` | 4 | Number of top performers to preserve unchanged |

**Training Modes:**

| Argument | Default | Description |
|----------|---------|-------------|
| `--mirror` | False | Force mirror matches (same deck vs same deck) |
| `--no_league` | False | Disable training against historical opponents |
| `--scripted_ratio F` | 0.2 | Ratio of games vs scripted opponents (0-1) |
| `--resume PATH` | None | Resume from checkpoint (e.g., `checkpoints/checkpoint_ep100.pt`) |
| `--verbose` | False | Enable detailed debug output |

**Example Commands:**

```bash
# Basic training (recommended starting point)
python train_advanced.py --episodes 5000 --no_pbt --num_workers 10

# Full GA/PBT training with 8-core CPU
python train_advanced.py --episodes 10000 --population 8 --num_workers 6 --mcts_sims 50

# Resume from checkpoint
python train_advanced.py --resume checkpoints/checkpoint_ep500.pt --episodes 10000

# Conservative settings (lower memory)
python train_advanced.py --episodes 5000 --num_workers 4 --mcts_sims 25 --batch_size 128
```

### 2. Monitor Training
In a separate terminal, visualize the metrics in real-time:

```bash
python plot_training.py --live
```

This will show:
*   Win Rate trends
*   Loss convergence
*   Strategic metrics (Prizes taken, Evolutions)
*   "Learning Events" (sudden jumps in performance)

### 3. Analyze & Inspect
Check how a specific card is implemented:

```bash
python inspect_card.py "Alakazam"
```

Verify that the latest model is actually learning good strategies:

```bash
python analyze_strategies_v2.py --model best_elo_policy.pt
# Then open strategy_visualizer.html to view the results
```

---


## üìä Tracking Metrics Explained

### 1. Basic Metrics (`plot_training.py`)
*   **Win Rate**: The rolling average win rate of Player 0.
    *   *Interpretation*: In self-play, this will naturally hover around **50%**. Do not panic if it doesn't rise; this simply means the agent is evenly matched with itself.
*   **Loss (Policy/Value)**: Measures how well the neural network predicts the MCTS search results.
    *   *Interpretation*: Should decrease rapidly at first, then plateau. A sudden drop often indicates a "breakthrough" in understanding.
*   **Game Length**: number of turns/actions per game.
    *   *Interpretation*: <20 usually means the agent is "suiciding" (decking out) to end the game fast. >50 means it is actually playing.

### 2. Strategic Metrics (The Real Progress Indicators)
*   **ELO Score**: A rating derived *only* from games played against the **League** (frozen past versions).
    *   *Interpretation*: This is your main "thermometer" for progress. If ELO is rising, the agent is beating its past self. If ELO is flat, it has stalled.
*   **Checkpoint Winrate**: Win rate from periodic evaluation games specifically against the *previous saved checkpoint*.
    *   *Interpretation*: Shown as green diamonds in the dashboard.
        *   **> 55%**: Agent is actively learning.
        *   **~ 50%**: Agent has plateaued.
        *   **< 45%**: Catastrophic forgetting (rare).
*   **Avg Prizes**: How many prize cards taken.
    *   *Interpretation*: 0 = doing nothing. 1-2 = attacking randomly. 5-6 = winning dominantly.
*   **Avg Evolutions**: Count of evolutions per game. Critical for setup decks.

### 3. Alpha-Rank (`evaluate_alpharank.py`)
**Alpha-Rank** is a sophisticated game-theoretic metric (Omidshafiei et al., 2019) used to handle non-transitive dynamics (Rock-Paper-Scissors).
*   **Why use it**: ELO assumes A > B and B > C implies A > C. This is false in Pok√©mon TCG (Deck types matter). Alpha-Rank identifies the true "Meta-Nash" equilibrium.
*   **When to check**: Alpha-Rank is **NOT** calculated during the main training loop (it's too slow). You must run `evaluate_alpharank.py` **offline** after saving multiple checkpoints. It will load all your saved agents and run a tournament between them to produce the final rankings.

### 4. Gauntlet Evaluation (Absolute Skill Test)
The **Gauntlet** runs automatically every 200 episodes during training. Unlike ELO/Alpha-Rank (relative metrics), it measures **absolute skill** against fixed, non-learning opponents.

**Output Example:**
```
üõ°Ô∏è --- EVALUATION GAUNTLET (Episode 400) ---
   vs Random      : ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    85% | Prizes: 2.3 | Attacks: 18% | Pass: 45% | DeckOuts: 2
   vs Aggressive  : ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      60% | Prizes: 1.8 | Attacks: 22% | Pass: 40% | DeckOuts: 0
   vs Defensive   : ‚ñà‚ñà‚ñà‚ñà        45% | Prizes: 1.2 | Attacks: 15% | Pass: 55% | DeckOuts: 1
   ‚úÖ HEALTHY: Agent is learning proper strategy!
```

**How to Interpret:**

| Metric | Meaning | Healthy Range |
|--------|---------|---------------|
| **vs Random** | Sanity check - must beat random | **>80%** |
| **vs Aggressive** | Strategy check - can it beat attackers | **>30%** |
| **Prizes** | Avg prizes taken per game (actual goal) | **>1.0** |
| **Attacks %** | Percentage of actions that are attacks | **10-25%** |
| **Pass %** | Percentage of actions that are pass | **30-60%** |
| **DeckOuts** | Wins by opponent running out of cards | **<5** |

**Diagnosing Problems:**

| Symptom | vs Random | Prizes | Pass % | Diagnosis |
|---------|-----------|--------|--------|-----------|
| Healthy | >85% | >2.0 | 40-60% | ‚úÖ Learning correctly |
| Suicide Bug | <50% | 0 | Any | ‚ùå Value head inverted |
| Passive Bug | 60-80% | 0 | 100% | ‚ùå Agent only passes |
| Farming Bug | 70% | <0.5 | 30% | ‚ùå Stalling for rewards |

---

## üéØ Observation Space (`obs_dim=229`)

The neural network receives a **229-dimensional normalized vector** representing the game state:

### Vector Breakdown:

| Section | Dims | Range | Description |
|---------|------|-------|-------------|
| **Global** | 5 | 0-1 | Turn number, prizes taken (me/opp), deck size (me/opp) |
| **Hand (BoW)** | 77 | 0-1 | Bag-of-words: count of each card type in hand (normalized) |
| **Opp Hand** | 1 | 0-1 | Opponent hand size (hidden info proxy) |
| **My Active** | 10 | 0-1 | Has pokemon, energy, damage, HP ratio, tool, 5√ó status |
| **My Bench** | 50 | 0-1 | 5 slots √ó 10 features each |
| **Opp Active** | 10 | 0-1 | Same as My Active |
| **Opp Bench** | 50 | 0-1 | Same as My Bench |
| **Opp Model** | 8 | 0-1 | Turn since supporter, evolution stage, threat level, etc. |
| **My Discard** | 9 | 0-1 | Pokemon/Trainer/Energy counts + key card counts |
| **Opp Discard** | 9 | 0-1 | Same as My Discard |

### Key Features:

**Bag-of-Words Hand Encoding (77 dims):**
The agent can "see" **which specific cards** it holds, not just counts. Uses a fixed vocabulary of 77 cards from the CARD_REGISTRY.

```
Example: Hand = ["Alakazam", "Rare Candy", "Rare Candy", "Basic Psychic Energy"]
         ‚Üí Vector has 0.25 at Alakazam index, 0.5 at Rare Candy index, 0.25 at Energy index
```

**Normalization:**
All values are normalized to 0-1 range to prevent gradient explosion:
- Damage: `damage / 300.0`
- Energy: `energy_count / 5.0`
- Deck size: `cards / 60.0`
- Turn: `turn / 50.0`

---

## üß† Technical Approach Details


### Parallelization Strategy
Unlike standard RL which often runs on a single thread, we use Python's `multiprocessing` (spawn context) to run full game simulations on separate CPU cores.
*   **Workers**: Execute episodes using the current policy weights.
*   **Main Process**: Collects experience trajectories, updates the Neural Network on the GPU, and handles the Genetic Algorithm steps (crossover/mutation).

### Genetic Algorithm (GA)
Instead of a single "best" agent, we maintain a population.
1.  **Evaluation**: Agents play against each other and the "League".
2.  **Selection**: Tournament selection based on fitness (Win Rate + Strategy Bonuses).
3.  **Evolution**: Winning weights are blended (crossover) and perturbed (mutation) to create the next generation.
4.  **Elitism**: The absolute best agents are preserved exactly to ensure monotonic improvement.

### Reward Shaping
The sparse reward of "winning +1 / losing -1" is too difficult for initial learning. We add **minimal, positive** dense rewards:
*   Development: Benching Pokemon (+0.05), Evolving (+0.1), Attaching Energy (+0.1).
*   Attacking: Using an attack action (+0.2).
*   **Prize Taking (+1.0)**: The primary goal - big reward for taking prizes.
*   Pass Tiebreaker: Tiny penalty (-0.01) just to break ties when other actions exist.

> **Note:** Heavy penalties (e.g., -2.0 for passing) cause "toxic" learning where agents suicide to avoid penalties. Keep rewards positive-focused.

---

## üéÆ Action Space (`n_actions=873`)

The agent chooses from a **discrete action space of 873 possible actions**. Invalid actions are masked out each turn.

### Action Types:

| Action Type | Count | Description |
|-------------|-------|-------------|
| `PASS` | 1 | End turn without further actions |
| `PLAY_BASIC_TO_BENCH` | ~100 | Play Basic Pokemon to bench slot 0-4 |
| `EVOLVE_ACTIVE` | ~30 | Evolve active Pokemon |
| `EVOLVE_BENCH` | ~150 | Evolve bench Pokemon (card √ó slot) |
| `ATTACH_ACTIVE` | ~15 | Attach energy/tool to active |
| `ATTACH_BENCH` | ~75 | Attach energy/tool to bench slot |
| `PLAY_TRAINER` | ~350 | Play trainer card (with target/discard choices) |
| `RETREAT_TO` | 5 | Switch active with bench slot |
| `ATTACK` | 16 | Use attack 0 or 1, with optional target |
| `ATTACK_MAGNITUDE` | 20 | Variable damage attacks (e.g., Gholdengo) |
| `USE_ACTIVE_ABILITY` | 7 | Activate ability with optional target |

### Action Masking:

Each turn, the environment computes which actions are **legal**:
- Can only play cards in hand
- Energy attachment limited to once per turn
- Attacks require sufficient energy
- Evolution requires waiting a turn after playing/evolving

```python
mask = env.action_mask()  # Returns np.array of shape (873,) with 0/1
valid_actions = np.where(mask > 0)[0]  # Indices of legal actions
```

### Trainer Discard Actions:

Some trainers require discarding cards (Ultra Ball, Superior Energy Retrieval). These have **composite action indices** encoding both the trainer and discard choices:

```python
Action("PLAY_TRAINER", a="Ultra Ball", b=target, c=discard1_idx, d=discard2_idx)
```

---

## üìã TODO / Known Limitations

Future improvements tracked here:

### Observation Space
- [x] **Vocabulary Scaling**: ~~Fixed 77-card vocabulary~~ ‚Üí Padded to 100 for future expansion
- [x] **Pokemon Identity**: ~~Slots only had stats~~ ‚Üí Now includes one-hot identity (which Pokemon)
- [x] **Known Opponent Cards**: Restricted searches (Ultra Ball, Nest Ball) now reveal to opponent's `known_hand`. Unrestricted (Quick Search) stay hidden.
- [ ] **Card Embeddings**: Replace one-hot with learned embeddings for better generalization
- [ ] **Attention over Hand**: Use transformer attention to process variable-length hands

### Action Space
- [x] **Deterministic Ordering**: ~~Dictionary iteration order~~ ‚Üí Now sorted keys for consistent action indices
- [ ] **Autoregressive Actions**: Split discard selection into separate heads to reduce combinatorial explosion:
  - Head 1: Select Card to Play
  - Head 2: Select Target
  - Head 3: Select Discard(s)
- [ ] **Dynamic Hand Indices**: Discard indices (c=4) may be invalid if hand shrinks. Consider relative indexing.

### Gameplay Logic
- [x] **Mulligan Penalty**: ~~Cheats Basic into hand~~ ‚Üí Now opponent draws 1 card per mulligan taken
- [x] **Evolution Sickness Edge Cases**: `turn_played` resets correctly (PokemonSlot() creates fresh slot)
- [x] **Prize Card Draws**: Multi-prize implemented (2 for ex/V, 3 for Mega)

### League & Evaluation
- [ ] **Prioritized Fictitious Self-Play (PFSP)**: Keep all checkpoints, prioritize opponents agent struggles against
- [ ] **Rock-Paper-Scissors Robustness**: Current ELO-based selection may discard counter-strategies
- [ ] **Human Benchmark**: Create interface for human vs AI evaluation
- [ ] **Tournament Mode**: Automated round-robin tournaments with multiple decks


### Training
- [x] **Curriculum Learning**: `--curriculum N` teaches the action chain (attack ‚Üí attach ‚Üí evolve) during first N episodes (50% of games)
- [x] **Opponent Diversity**: 7 scripted agents (Random, Aggressive, Defensive, Evolution Rush, Energy First, Control, Combo)
- [ ] **Multi-Deck Training**: Train single agent across multiple deck archetypes

---

## üéÆ Game Rules (Anti-Stall Modifications)

To prevent the AI from learning degenerate "stall until deck-out" strategies, the game engine includes these modifications:

### Timeout Rules
| Condition | Outcome |
|-----------|---------|
| Player takes 6 prizes | **WIN** (+10 reward) |
| Player cannot draw (deck empty) | **LOSS** (opponent wins) |
| Max turns reached (50-75) | **Winner = more prizes taken** |
| Tied prizes on timeout | **DRAW** (0 reward) |

### Why This Matters
- **Standard Pokemon TCG**: Deck-out is a valid win condition, leading agents to discover "stall" as optimal.
- **Our Modification**: Timeout victory goes to the attacker (more prizes), forcing the agent to learn attacking is the primary path to victory.
- **Analogy**: Similar to Chess's 50-move rule that prevents indefinite stalling.

### Max Turns Setting
Games are capped at **50-75 turns** to force fast-paced, aggressive gameplay. This can be adjusted in `tcg/env.py`.

---

## üîß Training Modes

### AlphaZero Mode (Default)
Uses MCTS + Policy-Value Network. Best for learning complex multi-step strategies.

```bash
python train_advanced.py --episodes 5000 --mcts_sims 25
```

### PPO Mode (Experimental)
Uses Proximal Policy Optimization without MCTS. Learns directly from reward shaping. Better for when MCTS discovers degenerate strategies.

```bash
python train_advanced.py --episodes 5000 --ppo
```

| Mode | Pros | Cons |
|------|------|------|
| **AlphaZero** | Deep lookahead, strategic planning | MCTS can find "stall" strategies |
| **PPO** | Learns directly from rewards, faster | No lookahead, more sample-hungry |

---

## ‚ö†Ô∏è Common Issues & Fixes

*   **"No module named torch"**: Ensure you are in the correct virtual environment.
    ```bash
    source pkm/bin/activate  # OR source venv/bin/activate
    ```
*   **High Memory Usage**: Reduce `--num_workers`. Each worker loads a full PyTorch model copy.
*   **Slow Training**: Decrease `--mcts_sims` (e.g., to 25) or increase batch size.
*   **Checkpoint Incompatible**: If `obs_dim` changed (new cards added), you must train from scratch or use `--resume` with matching architecture.
*   **Agent Stalls/Passes**: Increase `--curriculum` episodes or try `--ppo` mode.

